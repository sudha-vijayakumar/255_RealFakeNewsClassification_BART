{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Model Creation Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJRKKN2GRI2o"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
        "logging.root.level = logging.INFO "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0OkkMyFRI2s",
        "outputId": "928ea783-eeb6-4e22-c6f0-95abdc71713f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Doc2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPvQ-I86FYGt"
      },
      "source": [
        "### Pull BART embedding data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY4KPpJcWX0P",
        "outputId": "813db8c4-b622-4e6d-ddb8-96fe737db9d6"
      },
      "source": [
        "# Data resaved to local Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llEPIHIT536k",
        "outputId": "7c837abd-dad6-43a9-a502-e105c0573dc0"
      },
      "source": [
        "!wget \"https://raw.githubusercontent.com/255-DataMining/termproject/main/bart_doc_embeddings.zip?token=AEVQ7D44HLZW77XOZDW4R7C7VTX54\" -O bart_doc_embeddings.zip\n",
        "\n",
        "!unzip /content/drive/My\\ Drive/bart_doc_embeddings.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/bart_doc_embeddings.zip\n",
            "  inflating: bart_doc_embeddings     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlRGGDzy54DE"
      },
      "source": [
        "import pandas as pd\n",
        "embeddings_df = pd.read_pickle('bart_doc_embeddings')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejrVwlX854JS"
      },
      "source": [
        "target = embeddings_df['Target']\n",
        "del embeddings_df['Target'] # remove target column from dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9IcDJt1RI3O"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings_df, target, test_size = 0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "FiQw8Uc4RI3R",
        "outputId": "cafaa53e-7a94-4bd4-8047-67e5eba0011e"
      },
      "source": [
        "max_features = 20000\n",
        "tokenizer = Tokenizer(num_words = max_features)\n",
        "tokenizer.fit_on_texts(list(df['Text'].values))\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bf5833af69df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_zZKaMiRI3T"
      },
      "source": [
        "X_train = pad_sequences(X_train, maxlen = maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen = maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CEYqXjPRI3W"
      },
      "source": [
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "bPtZVYO5RI3Y",
        "outputId": "b6019218-73ce-432e-b1a3-861d51e724f4"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>984</th>\n",
              "      <th>985</th>\n",
              "      <th>986</th>\n",
              "      <th>987</th>\n",
              "      <th>988</th>\n",
              "      <th>989</th>\n",
              "      <th>990</th>\n",
              "      <th>991</th>\n",
              "      <th>992</th>\n",
              "      <th>993</th>\n",
              "      <th>994</th>\n",
              "      <th>995</th>\n",
              "      <th>996</th>\n",
              "      <th>997</th>\n",
              "      <th>998</th>\n",
              "      <th>999</th>\n",
              "      <th>1000</th>\n",
              "      <th>1001</th>\n",
              "      <th>1002</th>\n",
              "      <th>1003</th>\n",
              "      <th>1004</th>\n",
              "      <th>1005</th>\n",
              "      <th>1006</th>\n",
              "      <th>1007</th>\n",
              "      <th>1008</th>\n",
              "      <th>1009</th>\n",
              "      <th>1010</th>\n",
              "      <th>1011</th>\n",
              "      <th>1012</th>\n",
              "      <th>1013</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45440</th>\n",
              "      <td>-0.134450</td>\n",
              "      <td>0.262440</td>\n",
              "      <td>0.578321</td>\n",
              "      <td>-1.077683</td>\n",
              "      <td>-0.356566</td>\n",
              "      <td>-0.501722</td>\n",
              "      <td>-0.522065</td>\n",
              "      <td>0.074667</td>\n",
              "      <td>-0.342787</td>\n",
              "      <td>0.252209</td>\n",
              "      <td>0.388035</td>\n",
              "      <td>-0.199644</td>\n",
              "      <td>-0.136609</td>\n",
              "      <td>-0.501038</td>\n",
              "      <td>0.820075</td>\n",
              "      <td>0.135116</td>\n",
              "      <td>0.666309</td>\n",
              "      <td>-0.363225</td>\n",
              "      <td>0.178121</td>\n",
              "      <td>0.273393</td>\n",
              "      <td>4.074825</td>\n",
              "      <td>0.026011</td>\n",
              "      <td>0.690269</td>\n",
              "      <td>-0.110360</td>\n",
              "      <td>0.108706</td>\n",
              "      <td>-0.495936</td>\n",
              "      <td>-0.511397</td>\n",
              "      <td>-0.029927</td>\n",
              "      <td>0.072510</td>\n",
              "      <td>0.940371</td>\n",
              "      <td>-0.467790</td>\n",
              "      <td>-0.711587</td>\n",
              "      <td>-0.099095</td>\n",
              "      <td>-0.158908</td>\n",
              "      <td>-0.463326</td>\n",
              "      <td>-0.402371</td>\n",
              "      <td>-0.844128</td>\n",
              "      <td>0.116822</td>\n",
              "      <td>-0.297042</td>\n",
              "      <td>-0.266423</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.780134</td>\n",
              "      <td>-0.047374</td>\n",
              "      <td>0.420634</td>\n",
              "      <td>-0.244636</td>\n",
              "      <td>-0.423883</td>\n",
              "      <td>-0.154868</td>\n",
              "      <td>-0.390107</td>\n",
              "      <td>-0.010902</td>\n",
              "      <td>-0.761026</td>\n",
              "      <td>-0.121108</td>\n",
              "      <td>-0.484754</td>\n",
              "      <td>-0.751321</td>\n",
              "      <td>0.060033</td>\n",
              "      <td>-0.494115</td>\n",
              "      <td>0.773746</td>\n",
              "      <td>0.223569</td>\n",
              "      <td>-0.070448</td>\n",
              "      <td>-0.273847</td>\n",
              "      <td>0.146313</td>\n",
              "      <td>-0.001090</td>\n",
              "      <td>0.336629</td>\n",
              "      <td>-0.417566</td>\n",
              "      <td>-0.294803</td>\n",
              "      <td>-0.681473</td>\n",
              "      <td>0.463959</td>\n",
              "      <td>-0.396953</td>\n",
              "      <td>0.078303</td>\n",
              "      <td>-3.402322</td>\n",
              "      <td>0.006979</td>\n",
              "      <td>-0.481918</td>\n",
              "      <td>-0.483600</td>\n",
              "      <td>-0.137464</td>\n",
              "      <td>0.070677</td>\n",
              "      <td>1.301823</td>\n",
              "      <td>-0.478286</td>\n",
              "      <td>-0.343352</td>\n",
              "      <td>0.431994</td>\n",
              "      <td>-0.028658</td>\n",
              "      <td>0.590625</td>\n",
              "      <td>-0.374684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4682</th>\n",
              "      <td>-0.071097</td>\n",
              "      <td>0.650376</td>\n",
              "      <td>0.105896</td>\n",
              "      <td>-0.975207</td>\n",
              "      <td>-0.227172</td>\n",
              "      <td>-0.302817</td>\n",
              "      <td>-0.705155</td>\n",
              "      <td>0.258375</td>\n",
              "      <td>-0.335785</td>\n",
              "      <td>0.031485</td>\n",
              "      <td>-0.264432</td>\n",
              "      <td>0.262880</td>\n",
              "      <td>-0.383808</td>\n",
              "      <td>0.042080</td>\n",
              "      <td>0.222424</td>\n",
              "      <td>-0.035038</td>\n",
              "      <td>0.220420</td>\n",
              "      <td>-0.274977</td>\n",
              "      <td>0.569885</td>\n",
              "      <td>0.446317</td>\n",
              "      <td>4.058769</td>\n",
              "      <td>0.173747</td>\n",
              "      <td>0.219695</td>\n",
              "      <td>-0.410815</td>\n",
              "      <td>-0.396934</td>\n",
              "      <td>-0.631219</td>\n",
              "      <td>-0.561822</td>\n",
              "      <td>-0.339196</td>\n",
              "      <td>-0.010965</td>\n",
              "      <td>0.202776</td>\n",
              "      <td>0.039486</td>\n",
              "      <td>-0.572300</td>\n",
              "      <td>0.457579</td>\n",
              "      <td>0.198336</td>\n",
              "      <td>-0.373248</td>\n",
              "      <td>-0.141867</td>\n",
              "      <td>-0.835898</td>\n",
              "      <td>-0.313361</td>\n",
              "      <td>-0.328249</td>\n",
              "      <td>-0.388132</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.322029</td>\n",
              "      <td>0.069437</td>\n",
              "      <td>0.592489</td>\n",
              "      <td>-0.126234</td>\n",
              "      <td>-0.179142</td>\n",
              "      <td>-0.207534</td>\n",
              "      <td>-0.549950</td>\n",
              "      <td>0.209278</td>\n",
              "      <td>-0.324933</td>\n",
              "      <td>-0.096700</td>\n",
              "      <td>-0.744409</td>\n",
              "      <td>-0.681939</td>\n",
              "      <td>-0.075660</td>\n",
              "      <td>-0.288228</td>\n",
              "      <td>0.473865</td>\n",
              "      <td>-0.125110</td>\n",
              "      <td>-0.095247</td>\n",
              "      <td>-0.163965</td>\n",
              "      <td>-0.066139</td>\n",
              "      <td>0.214853</td>\n",
              "      <td>0.085618</td>\n",
              "      <td>-0.242203</td>\n",
              "      <td>-0.416534</td>\n",
              "      <td>-0.659860</td>\n",
              "      <td>0.062063</td>\n",
              "      <td>0.124640</td>\n",
              "      <td>0.384943</td>\n",
              "      <td>-4.263932</td>\n",
              "      <td>0.250174</td>\n",
              "      <td>-0.185871</td>\n",
              "      <td>-0.637177</td>\n",
              "      <td>-0.480210</td>\n",
              "      <td>0.340123</td>\n",
              "      <td>1.285411</td>\n",
              "      <td>-1.049078</td>\n",
              "      <td>-0.914834</td>\n",
              "      <td>-0.563537</td>\n",
              "      <td>-0.244260</td>\n",
              "      <td>0.535899</td>\n",
              "      <td>-0.951509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2552</th>\n",
              "      <td>-0.064202</td>\n",
              "      <td>0.351255</td>\n",
              "      <td>0.179642</td>\n",
              "      <td>-1.092753</td>\n",
              "      <td>-0.236207</td>\n",
              "      <td>-0.495307</td>\n",
              "      <td>-0.545382</td>\n",
              "      <td>-0.010428</td>\n",
              "      <td>-0.380019</td>\n",
              "      <td>0.084189</td>\n",
              "      <td>-0.050136</td>\n",
              "      <td>-0.247001</td>\n",
              "      <td>-0.074742</td>\n",
              "      <td>0.162428</td>\n",
              "      <td>0.268178</td>\n",
              "      <td>-0.453305</td>\n",
              "      <td>0.268456</td>\n",
              "      <td>0.071911</td>\n",
              "      <td>0.705507</td>\n",
              "      <td>0.600100</td>\n",
              "      <td>4.130463</td>\n",
              "      <td>0.581900</td>\n",
              "      <td>0.286419</td>\n",
              "      <td>-0.500350</td>\n",
              "      <td>-0.293510</td>\n",
              "      <td>-0.416525</td>\n",
              "      <td>-0.898593</td>\n",
              "      <td>-0.500154</td>\n",
              "      <td>0.379782</td>\n",
              "      <td>0.158139</td>\n",
              "      <td>0.100749</td>\n",
              "      <td>-0.897189</td>\n",
              "      <td>-0.116459</td>\n",
              "      <td>0.398995</td>\n",
              "      <td>-0.438727</td>\n",
              "      <td>-0.250573</td>\n",
              "      <td>-0.764475</td>\n",
              "      <td>-0.107354</td>\n",
              "      <td>-0.310506</td>\n",
              "      <td>-0.122787</td>\n",
              "      <td>...</td>\n",
              "      <td>0.178988</td>\n",
              "      <td>-0.348918</td>\n",
              "      <td>0.267478</td>\n",
              "      <td>-0.269511</td>\n",
              "      <td>-0.260660</td>\n",
              "      <td>-0.230779</td>\n",
              "      <td>-0.776124</td>\n",
              "      <td>0.063705</td>\n",
              "      <td>-0.328794</td>\n",
              "      <td>-0.144679</td>\n",
              "      <td>-0.516167</td>\n",
              "      <td>-0.499578</td>\n",
              "      <td>0.296119</td>\n",
              "      <td>-0.429111</td>\n",
              "      <td>0.460954</td>\n",
              "      <td>0.361915</td>\n",
              "      <td>-0.130458</td>\n",
              "      <td>0.148237</td>\n",
              "      <td>-0.178780</td>\n",
              "      <td>0.204167</td>\n",
              "      <td>-0.037812</td>\n",
              "      <td>-0.013201</td>\n",
              "      <td>-0.528037</td>\n",
              "      <td>-0.839780</td>\n",
              "      <td>0.506832</td>\n",
              "      <td>0.184121</td>\n",
              "      <td>0.298673</td>\n",
              "      <td>-4.292791</td>\n",
              "      <td>0.164138</td>\n",
              "      <td>-0.424425</td>\n",
              "      <td>-0.470607</td>\n",
              "      <td>-0.148486</td>\n",
              "      <td>0.304992</td>\n",
              "      <td>1.216745</td>\n",
              "      <td>-0.588296</td>\n",
              "      <td>-0.647081</td>\n",
              "      <td>-0.425242</td>\n",
              "      <td>0.033832</td>\n",
              "      <td>0.289780</td>\n",
              "      <td>-0.789723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63405</th>\n",
              "      <td>-0.069562</td>\n",
              "      <td>1.002866</td>\n",
              "      <td>0.472012</td>\n",
              "      <td>-1.147172</td>\n",
              "      <td>-0.106377</td>\n",
              "      <td>-0.280113</td>\n",
              "      <td>-0.404112</td>\n",
              "      <td>-0.337914</td>\n",
              "      <td>-0.309353</td>\n",
              "      <td>0.583253</td>\n",
              "      <td>0.308521</td>\n",
              "      <td>-0.060384</td>\n",
              "      <td>-0.242463</td>\n",
              "      <td>-0.262891</td>\n",
              "      <td>0.513804</td>\n",
              "      <td>0.059984</td>\n",
              "      <td>0.232748</td>\n",
              "      <td>0.122221</td>\n",
              "      <td>0.624330</td>\n",
              "      <td>0.596456</td>\n",
              "      <td>3.907214</td>\n",
              "      <td>0.148736</td>\n",
              "      <td>0.180884</td>\n",
              "      <td>-0.312983</td>\n",
              "      <td>-0.057843</td>\n",
              "      <td>-0.637491</td>\n",
              "      <td>-0.779275</td>\n",
              "      <td>-0.519098</td>\n",
              "      <td>-0.294131</td>\n",
              "      <td>0.137462</td>\n",
              "      <td>-0.237275</td>\n",
              "      <td>-0.626726</td>\n",
              "      <td>-0.082022</td>\n",
              "      <td>0.163667</td>\n",
              "      <td>-0.268591</td>\n",
              "      <td>-0.673653</td>\n",
              "      <td>-0.827068</td>\n",
              "      <td>-0.235209</td>\n",
              "      <td>0.053536</td>\n",
              "      <td>-0.013168</td>\n",
              "      <td>...</td>\n",
              "      <td>0.074356</td>\n",
              "      <td>-0.003652</td>\n",
              "      <td>0.584716</td>\n",
              "      <td>-0.057593</td>\n",
              "      <td>-0.464828</td>\n",
              "      <td>-0.331034</td>\n",
              "      <td>-0.325581</td>\n",
              "      <td>0.320217</td>\n",
              "      <td>-0.446186</td>\n",
              "      <td>-0.187984</td>\n",
              "      <td>0.147887</td>\n",
              "      <td>-0.547328</td>\n",
              "      <td>0.113338</td>\n",
              "      <td>-0.167756</td>\n",
              "      <td>0.649093</td>\n",
              "      <td>-0.006725</td>\n",
              "      <td>0.233883</td>\n",
              "      <td>-0.156380</td>\n",
              "      <td>-0.011423</td>\n",
              "      <td>0.348039</td>\n",
              "      <td>-0.563998</td>\n",
              "      <td>-0.224916</td>\n",
              "      <td>-0.278807</td>\n",
              "      <td>-0.451333</td>\n",
              "      <td>0.388905</td>\n",
              "      <td>0.556542</td>\n",
              "      <td>0.534480</td>\n",
              "      <td>-4.942526</td>\n",
              "      <td>-0.215233</td>\n",
              "      <td>-0.296609</td>\n",
              "      <td>-0.470443</td>\n",
              "      <td>-0.296959</td>\n",
              "      <td>0.192390</td>\n",
              "      <td>1.367222</td>\n",
              "      <td>-0.556850</td>\n",
              "      <td>-0.315990</td>\n",
              "      <td>-0.033124</td>\n",
              "      <td>0.214176</td>\n",
              "      <td>0.462258</td>\n",
              "      <td>-0.015277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62660</th>\n",
              "      <td>0.369185</td>\n",
              "      <td>0.699991</td>\n",
              "      <td>-0.121170</td>\n",
              "      <td>-0.544654</td>\n",
              "      <td>-0.107379</td>\n",
              "      <td>-0.636448</td>\n",
              "      <td>-0.379480</td>\n",
              "      <td>-0.090207</td>\n",
              "      <td>-0.646012</td>\n",
              "      <td>0.069929</td>\n",
              "      <td>-0.590505</td>\n",
              "      <td>-0.025946</td>\n",
              "      <td>0.164470</td>\n",
              "      <td>0.055385</td>\n",
              "      <td>0.486047</td>\n",
              "      <td>-0.036013</td>\n",
              "      <td>0.192093</td>\n",
              "      <td>-0.168957</td>\n",
              "      <td>0.575531</td>\n",
              "      <td>0.047989</td>\n",
              "      <td>3.662821</td>\n",
              "      <td>0.261048</td>\n",
              "      <td>0.733527</td>\n",
              "      <td>-0.237862</td>\n",
              "      <td>-0.313786</td>\n",
              "      <td>-0.180522</td>\n",
              "      <td>-0.227655</td>\n",
              "      <td>-0.684802</td>\n",
              "      <td>0.006642</td>\n",
              "      <td>0.707173</td>\n",
              "      <td>0.636501</td>\n",
              "      <td>-0.747069</td>\n",
              "      <td>-0.200026</td>\n",
              "      <td>0.568793</td>\n",
              "      <td>-0.561503</td>\n",
              "      <td>-0.440175</td>\n",
              "      <td>-0.774842</td>\n",
              "      <td>-0.467958</td>\n",
              "      <td>-0.147768</td>\n",
              "      <td>-0.316719</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.279264</td>\n",
              "      <td>-0.008654</td>\n",
              "      <td>0.760246</td>\n",
              "      <td>-0.149547</td>\n",
              "      <td>-0.125908</td>\n",
              "      <td>-0.368738</td>\n",
              "      <td>-0.346827</td>\n",
              "      <td>-0.248078</td>\n",
              "      <td>-0.254094</td>\n",
              "      <td>0.010780</td>\n",
              "      <td>-0.532663</td>\n",
              "      <td>-0.951509</td>\n",
              "      <td>-0.218478</td>\n",
              "      <td>-0.674714</td>\n",
              "      <td>0.850886</td>\n",
              "      <td>0.417761</td>\n",
              "      <td>-0.469728</td>\n",
              "      <td>0.055989</td>\n",
              "      <td>0.011159</td>\n",
              "      <td>0.222580</td>\n",
              "      <td>0.571447</td>\n",
              "      <td>-0.395614</td>\n",
              "      <td>-0.340764</td>\n",
              "      <td>-0.481868</td>\n",
              "      <td>0.428434</td>\n",
              "      <td>-0.112780</td>\n",
              "      <td>0.337657</td>\n",
              "      <td>-3.295645</td>\n",
              "      <td>0.193986</td>\n",
              "      <td>-0.156779</td>\n",
              "      <td>-0.639332</td>\n",
              "      <td>-0.081229</td>\n",
              "      <td>0.293695</td>\n",
              "      <td>2.023443</td>\n",
              "      <td>-0.298713</td>\n",
              "      <td>-0.519224</td>\n",
              "      <td>-0.152782</td>\n",
              "      <td>0.162731</td>\n",
              "      <td>0.687542</td>\n",
              "      <td>-0.387913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40730</th>\n",
              "      <td>-0.415217</td>\n",
              "      <td>0.302802</td>\n",
              "      <td>0.363305</td>\n",
              "      <td>-0.676308</td>\n",
              "      <td>-0.145534</td>\n",
              "      <td>-0.271717</td>\n",
              "      <td>-0.773202</td>\n",
              "      <td>0.057465</td>\n",
              "      <td>-0.632910</td>\n",
              "      <td>0.114273</td>\n",
              "      <td>-0.985415</td>\n",
              "      <td>0.192508</td>\n",
              "      <td>0.140767</td>\n",
              "      <td>-0.157992</td>\n",
              "      <td>0.236175</td>\n",
              "      <td>-0.230375</td>\n",
              "      <td>0.116887</td>\n",
              "      <td>-0.407898</td>\n",
              "      <td>0.720863</td>\n",
              "      <td>0.188261</td>\n",
              "      <td>3.986844</td>\n",
              "      <td>0.094604</td>\n",
              "      <td>0.551638</td>\n",
              "      <td>-0.291581</td>\n",
              "      <td>-0.468416</td>\n",
              "      <td>-0.564269</td>\n",
              "      <td>-0.418292</td>\n",
              "      <td>-0.775880</td>\n",
              "      <td>0.046401</td>\n",
              "      <td>0.543350</td>\n",
              "      <td>0.571659</td>\n",
              "      <td>-0.808918</td>\n",
              "      <td>-0.580970</td>\n",
              "      <td>0.496247</td>\n",
              "      <td>-0.638677</td>\n",
              "      <td>-0.298393</td>\n",
              "      <td>-0.408845</td>\n",
              "      <td>-0.395726</td>\n",
              "      <td>-0.016324</td>\n",
              "      <td>-0.485775</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080817</td>\n",
              "      <td>-0.064906</td>\n",
              "      <td>0.709327</td>\n",
              "      <td>0.318874</td>\n",
              "      <td>-0.480154</td>\n",
              "      <td>-0.384177</td>\n",
              "      <td>-0.196702</td>\n",
              "      <td>-0.009017</td>\n",
              "      <td>0.061070</td>\n",
              "      <td>-0.021810</td>\n",
              "      <td>-0.360412</td>\n",
              "      <td>-1.083120</td>\n",
              "      <td>0.010872</td>\n",
              "      <td>-0.972788</td>\n",
              "      <td>0.920809</td>\n",
              "      <td>-0.093391</td>\n",
              "      <td>-0.799613</td>\n",
              "      <td>0.099821</td>\n",
              "      <td>-0.115048</td>\n",
              "      <td>0.485788</td>\n",
              "      <td>0.041788</td>\n",
              "      <td>-0.395237</td>\n",
              "      <td>-0.554667</td>\n",
              "      <td>-0.711819</td>\n",
              "      <td>0.244467</td>\n",
              "      <td>0.073103</td>\n",
              "      <td>0.358947</td>\n",
              "      <td>-3.820111</td>\n",
              "      <td>0.291138</td>\n",
              "      <td>-0.015004</td>\n",
              "      <td>-0.540778</td>\n",
              "      <td>-0.060070</td>\n",
              "      <td>-0.104363</td>\n",
              "      <td>1.693196</td>\n",
              "      <td>-0.564379</td>\n",
              "      <td>-0.446705</td>\n",
              "      <td>-0.431482</td>\n",
              "      <td>0.109963</td>\n",
              "      <td>0.448953</td>\n",
              "      <td>-0.610733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9046</th>\n",
              "      <td>-0.182313</td>\n",
              "      <td>0.042820</td>\n",
              "      <td>0.466055</td>\n",
              "      <td>-0.880667</td>\n",
              "      <td>-0.189104</td>\n",
              "      <td>0.080661</td>\n",
              "      <td>-0.199498</td>\n",
              "      <td>-0.000241</td>\n",
              "      <td>-0.170163</td>\n",
              "      <td>0.080747</td>\n",
              "      <td>-0.058362</td>\n",
              "      <td>0.115307</td>\n",
              "      <td>0.273897</td>\n",
              "      <td>-0.508497</td>\n",
              "      <td>0.124262</td>\n",
              "      <td>0.058112</td>\n",
              "      <td>0.478186</td>\n",
              "      <td>-0.207212</td>\n",
              "      <td>0.162872</td>\n",
              "      <td>-0.016636</td>\n",
              "      <td>4.131534</td>\n",
              "      <td>0.435700</td>\n",
              "      <td>-0.219006</td>\n",
              "      <td>-0.170878</td>\n",
              "      <td>-0.129281</td>\n",
              "      <td>-0.534437</td>\n",
              "      <td>-1.022682</td>\n",
              "      <td>-0.260133</td>\n",
              "      <td>-0.248073</td>\n",
              "      <td>0.912503</td>\n",
              "      <td>0.108745</td>\n",
              "      <td>-0.893017</td>\n",
              "      <td>0.249485</td>\n",
              "      <td>0.077965</td>\n",
              "      <td>0.115986</td>\n",
              "      <td>-0.723110</td>\n",
              "      <td>-0.857624</td>\n",
              "      <td>-0.033585</td>\n",
              "      <td>-0.219987</td>\n",
              "      <td>0.356225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.592973</td>\n",
              "      <td>-0.065757</td>\n",
              "      <td>-0.123317</td>\n",
              "      <td>0.019728</td>\n",
              "      <td>0.039752</td>\n",
              "      <td>-0.039714</td>\n",
              "      <td>-0.429986</td>\n",
              "      <td>0.375586</td>\n",
              "      <td>0.039241</td>\n",
              "      <td>0.014944</td>\n",
              "      <td>-0.187238</td>\n",
              "      <td>-0.521623</td>\n",
              "      <td>-0.730597</td>\n",
              "      <td>-0.229932</td>\n",
              "      <td>0.389347</td>\n",
              "      <td>0.105259</td>\n",
              "      <td>-0.455747</td>\n",
              "      <td>0.454199</td>\n",
              "      <td>0.015150</td>\n",
              "      <td>0.185843</td>\n",
              "      <td>-0.041509</td>\n",
              "      <td>-0.432467</td>\n",
              "      <td>-0.232712</td>\n",
              "      <td>-0.844573</td>\n",
              "      <td>0.221827</td>\n",
              "      <td>0.531325</td>\n",
              "      <td>0.249428</td>\n",
              "      <td>-4.781097</td>\n",
              "      <td>-0.843055</td>\n",
              "      <td>-0.271550</td>\n",
              "      <td>-0.075314</td>\n",
              "      <td>-0.398184</td>\n",
              "      <td>0.085101</td>\n",
              "      <td>1.121555</td>\n",
              "      <td>-0.323763</td>\n",
              "      <td>-0.333945</td>\n",
              "      <td>-0.882787</td>\n",
              "      <td>0.099782</td>\n",
              "      <td>0.302949</td>\n",
              "      <td>-0.603432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82882</th>\n",
              "      <td>0.147611</td>\n",
              "      <td>0.851922</td>\n",
              "      <td>0.316986</td>\n",
              "      <td>-0.209094</td>\n",
              "      <td>-0.122375</td>\n",
              "      <td>-0.219480</td>\n",
              "      <td>-0.374897</td>\n",
              "      <td>-0.441904</td>\n",
              "      <td>-0.124203</td>\n",
              "      <td>-0.282153</td>\n",
              "      <td>-0.387342</td>\n",
              "      <td>-0.172673</td>\n",
              "      <td>0.236830</td>\n",
              "      <td>-0.218112</td>\n",
              "      <td>-0.086526</td>\n",
              "      <td>-0.232781</td>\n",
              "      <td>0.564235</td>\n",
              "      <td>0.065213</td>\n",
              "      <td>0.482824</td>\n",
              "      <td>0.381044</td>\n",
              "      <td>4.043043</td>\n",
              "      <td>0.264262</td>\n",
              "      <td>0.127800</td>\n",
              "      <td>-0.246912</td>\n",
              "      <td>-0.087995</td>\n",
              "      <td>0.016336</td>\n",
              "      <td>-0.300619</td>\n",
              "      <td>0.346814</td>\n",
              "      <td>0.174088</td>\n",
              "      <td>0.144444</td>\n",
              "      <td>0.272871</td>\n",
              "      <td>-1.131625</td>\n",
              "      <td>0.352620</td>\n",
              "      <td>0.615097</td>\n",
              "      <td>0.456432</td>\n",
              "      <td>-0.345310</td>\n",
              "      <td>-0.415783</td>\n",
              "      <td>-0.060105</td>\n",
              "      <td>-0.171042</td>\n",
              "      <td>0.197192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.346960</td>\n",
              "      <td>0.190207</td>\n",
              "      <td>0.562024</td>\n",
              "      <td>0.126555</td>\n",
              "      <td>-0.010595</td>\n",
              "      <td>-0.352196</td>\n",
              "      <td>-0.814093</td>\n",
              "      <td>-0.096698</td>\n",
              "      <td>0.298005</td>\n",
              "      <td>-0.308784</td>\n",
              "      <td>-0.194096</td>\n",
              "      <td>-0.141109</td>\n",
              "      <td>-0.174674</td>\n",
              "      <td>-0.512709</td>\n",
              "      <td>0.271494</td>\n",
              "      <td>-0.071499</td>\n",
              "      <td>-0.354628</td>\n",
              "      <td>-0.128696</td>\n",
              "      <td>-0.307792</td>\n",
              "      <td>0.024591</td>\n",
              "      <td>0.543352</td>\n",
              "      <td>-0.004273</td>\n",
              "      <td>-0.044091</td>\n",
              "      <td>-0.732727</td>\n",
              "      <td>0.372102</td>\n",
              "      <td>-0.015039</td>\n",
              "      <td>0.407107</td>\n",
              "      <td>-5.079906</td>\n",
              "      <td>-0.158892</td>\n",
              "      <td>-0.518150</td>\n",
              "      <td>-0.360574</td>\n",
              "      <td>-0.398439</td>\n",
              "      <td>0.002499</td>\n",
              "      <td>0.374396</td>\n",
              "      <td>-0.438582</td>\n",
              "      <td>0.126341</td>\n",
              "      <td>-0.566207</td>\n",
              "      <td>-0.016939</td>\n",
              "      <td>0.280277</td>\n",
              "      <td>-0.399050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59614</th>\n",
              "      <td>0.589006</td>\n",
              "      <td>0.581548</td>\n",
              "      <td>0.261635</td>\n",
              "      <td>-0.450407</td>\n",
              "      <td>-0.097996</td>\n",
              "      <td>-0.220705</td>\n",
              "      <td>-0.301081</td>\n",
              "      <td>-0.138719</td>\n",
              "      <td>-0.569250</td>\n",
              "      <td>0.841387</td>\n",
              "      <td>0.118378</td>\n",
              "      <td>0.181328</td>\n",
              "      <td>0.233168</td>\n",
              "      <td>-0.766456</td>\n",
              "      <td>-0.042623</td>\n",
              "      <td>-0.330753</td>\n",
              "      <td>0.277856</td>\n",
              "      <td>0.152141</td>\n",
              "      <td>0.438471</td>\n",
              "      <td>-0.032096</td>\n",
              "      <td>4.077712</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>0.499545</td>\n",
              "      <td>0.014503</td>\n",
              "      <td>-0.126484</td>\n",
              "      <td>-0.120753</td>\n",
              "      <td>-0.051029</td>\n",
              "      <td>-0.355527</td>\n",
              "      <td>-0.011643</td>\n",
              "      <td>0.369066</td>\n",
              "      <td>-0.180364</td>\n",
              "      <td>-0.303859</td>\n",
              "      <td>-0.423275</td>\n",
              "      <td>0.039865</td>\n",
              "      <td>-0.121031</td>\n",
              "      <td>-0.370176</td>\n",
              "      <td>-0.835359</td>\n",
              "      <td>0.525526</td>\n",
              "      <td>-0.768723</td>\n",
              "      <td>-0.026574</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.342495</td>\n",
              "      <td>-0.158360</td>\n",
              "      <td>0.474479</td>\n",
              "      <td>-0.197857</td>\n",
              "      <td>-0.067949</td>\n",
              "      <td>0.006786</td>\n",
              "      <td>-0.150842</td>\n",
              "      <td>-0.529444</td>\n",
              "      <td>-0.347149</td>\n",
              "      <td>-0.089412</td>\n",
              "      <td>-1.131629</td>\n",
              "      <td>-0.334495</td>\n",
              "      <td>-0.261306</td>\n",
              "      <td>-0.290078</td>\n",
              "      <td>0.369793</td>\n",
              "      <td>0.441167</td>\n",
              "      <td>-0.525307</td>\n",
              "      <td>-0.175908</td>\n",
              "      <td>-0.032978</td>\n",
              "      <td>0.312400</td>\n",
              "      <td>0.286940</td>\n",
              "      <td>-0.276028</td>\n",
              "      <td>-0.250295</td>\n",
              "      <td>-0.925275</td>\n",
              "      <td>-0.281294</td>\n",
              "      <td>-0.016450</td>\n",
              "      <td>0.555824</td>\n",
              "      <td>-4.137691</td>\n",
              "      <td>0.078558</td>\n",
              "      <td>-0.583887</td>\n",
              "      <td>-0.587243</td>\n",
              "      <td>-0.940626</td>\n",
              "      <td>-0.028452</td>\n",
              "      <td>0.815175</td>\n",
              "      <td>-0.294802</td>\n",
              "      <td>-0.212270</td>\n",
              "      <td>-0.213610</td>\n",
              "      <td>-0.082450</td>\n",
              "      <td>0.226088</td>\n",
              "      <td>-0.400353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45525</th>\n",
              "      <td>-0.028653</td>\n",
              "      <td>0.815335</td>\n",
              "      <td>-0.070661</td>\n",
              "      <td>-0.568956</td>\n",
              "      <td>0.028497</td>\n",
              "      <td>-0.182420</td>\n",
              "      <td>-0.645451</td>\n",
              "      <td>-0.016486</td>\n",
              "      <td>-0.233741</td>\n",
              "      <td>0.180612</td>\n",
              "      <td>-0.018637</td>\n",
              "      <td>0.323453</td>\n",
              "      <td>-0.061543</td>\n",
              "      <td>-0.266488</td>\n",
              "      <td>0.049632</td>\n",
              "      <td>0.112574</td>\n",
              "      <td>0.339681</td>\n",
              "      <td>-0.284456</td>\n",
              "      <td>0.710098</td>\n",
              "      <td>0.349647</td>\n",
              "      <td>3.988703</td>\n",
              "      <td>0.236536</td>\n",
              "      <td>0.768851</td>\n",
              "      <td>-0.125630</td>\n",
              "      <td>-0.211876</td>\n",
              "      <td>-0.287001</td>\n",
              "      <td>-0.446071</td>\n",
              "      <td>-0.074443</td>\n",
              "      <td>-0.205135</td>\n",
              "      <td>0.714483</td>\n",
              "      <td>0.074147</td>\n",
              "      <td>-0.765576</td>\n",
              "      <td>-0.207790</td>\n",
              "      <td>0.163553</td>\n",
              "      <td>-0.009077</td>\n",
              "      <td>-0.416640</td>\n",
              "      <td>-0.793052</td>\n",
              "      <td>-0.140561</td>\n",
              "      <td>0.144829</td>\n",
              "      <td>-0.078203</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.846161</td>\n",
              "      <td>0.111554</td>\n",
              "      <td>0.663440</td>\n",
              "      <td>-0.273898</td>\n",
              "      <td>-0.088876</td>\n",
              "      <td>-0.400589</td>\n",
              "      <td>-0.600280</td>\n",
              "      <td>-0.347594</td>\n",
              "      <td>-0.153472</td>\n",
              "      <td>-0.189601</td>\n",
              "      <td>-0.230640</td>\n",
              "      <td>-0.935867</td>\n",
              "      <td>0.429099</td>\n",
              "      <td>-0.528861</td>\n",
              "      <td>0.435607</td>\n",
              "      <td>-0.268151</td>\n",
              "      <td>-0.259002</td>\n",
              "      <td>-0.066968</td>\n",
              "      <td>-0.155975</td>\n",
              "      <td>0.187275</td>\n",
              "      <td>0.530572</td>\n",
              "      <td>-0.569026</td>\n",
              "      <td>-0.511661</td>\n",
              "      <td>-0.614640</td>\n",
              "      <td>0.086440</td>\n",
              "      <td>-0.111650</td>\n",
              "      <td>0.596799</td>\n",
              "      <td>-5.639348</td>\n",
              "      <td>0.049255</td>\n",
              "      <td>-0.287720</td>\n",
              "      <td>-0.688298</td>\n",
              "      <td>-0.349942</td>\n",
              "      <td>0.107582</td>\n",
              "      <td>1.074157</td>\n",
              "      <td>-0.327401</td>\n",
              "      <td>-0.241292</td>\n",
              "      <td>-0.040961</td>\n",
              "      <td>0.509738</td>\n",
              "      <td>0.658524</td>\n",
              "      <td>-0.512060</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>71918 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2     ...      1021      1022      1023\n",
              "45440 -0.134450  0.262440  0.578321  ... -0.028658  0.590625 -0.374684\n",
              "4682  -0.071097  0.650376  0.105896  ... -0.244260  0.535899 -0.951509\n",
              "2552  -0.064202  0.351255  0.179642  ...  0.033832  0.289780 -0.789723\n",
              "63405 -0.069562  1.002866  0.472012  ...  0.214176  0.462258 -0.015277\n",
              "62660  0.369185  0.699991 -0.121170  ...  0.162731  0.687542 -0.387913\n",
              "...         ...       ...       ...  ...       ...       ...       ...\n",
              "40730 -0.415217  0.302802  0.363305  ...  0.109963  0.448953 -0.610733\n",
              "9046  -0.182313  0.042820  0.466055  ...  0.099782  0.302949 -0.603432\n",
              "82882  0.147611  0.851922  0.316986  ... -0.016939  0.280277 -0.399050\n",
              "59614  0.589006  0.581548  0.261635  ... -0.082450  0.226088 -0.400353\n",
              "45525 -0.028653  0.815335 -0.070661  ...  0.509738  0.658524 -0.512060\n",
              "\n",
              "[71918 rows x 1024 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioIxcsj-RI3a"
      },
      "source": [
        "## Model creation below using Keras library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VwBxjOlRI3a"
      },
      "source": [
        "# Define needed variables\n",
        "embed_size = 200\n",
        "maxlen = 2000\n",
        "# Importing needed keras libraries\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, GRU \n",
        "from keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPool1D, concatenate\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe_3KtCWRI3d"
      },
      "source": [
        "### NN model with BART embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLxhV6afRI3d",
        "outputId": "a6263169-4d1e-452e-a0de-49a62be8eafc"
      },
      "source": [
        "# Trying sequential model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.InputLayer(input_shape=X_train.shape[1]))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization()) # Batch normalization\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization()) # Batch normalization\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization()) # Batch normalization\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization()) # Batch normalization\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.BatchNormalization()) # Batch normalization\n",
        "model.add(Dense(64, activation='relu')) # 4 dense layers\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss =\"binary_crossentropy\", optimizer='adam', metrics = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "model.summary()\n",
        "\n",
        "# Credit to: https://realpython.com/python-keras-text-classification/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 87,745\n",
            "Trainable params: 87,105\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        },
        "id": "34Xrh-VuQcFR",
        "outputId": "d36e8629-2ea5-4d34-db40-d1ea0ec13232"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir='LR')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "finIelulrWdj"
      },
      "source": [
        "### Train the model for 25 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4XuZVOURI3f",
        "scrolled": true,
        "outputId": "966fa16b-65bb-4c9a-b7c4-a479da7df950"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size = 64, epochs = 25, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.1818 - accuracy: 0.9253 - precision_6: 0.9214 - recall_6: 0.9260 - val_loss: 0.1864 - val_accuracy: 0.9178 - val_precision_6: 0.8625 - val_recall_6: 0.9898\n",
            "Epoch 2/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.1232 - accuracy: 0.9524 - precision_6: 0.9471 - recall_6: 0.9559 - val_loss: 0.1066 - val_accuracy: 0.9588 - val_precision_6: 0.9621 - val_recall_6: 0.9534\n",
            "Epoch 3/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.1064 - accuracy: 0.9591 - precision_6: 0.9552 - recall_6: 0.9613 - val_loss: 0.1074 - val_accuracy: 0.9569 - val_precision_6: 0.9583 - val_recall_6: 0.9534\n",
            "Epoch 4/25\n",
            "1012/1012 [==============================] - 4s 4ms/step - loss: 0.0978 - accuracy: 0.9627 - precision_6: 0.9598 - recall_6: 0.9639 - val_loss: 0.1197 - val_accuracy: 0.9518 - val_precision_6: 0.9573 - val_recall_6: 0.9434\n",
            "Epoch 5/25\n",
            "1012/1012 [==============================] - 4s 4ms/step - loss: 0.0839 - accuracy: 0.9681 - precision_6: 0.9662 - recall_6: 0.9685 - val_loss: 0.1097 - val_accuracy: 0.9568 - val_precision_6: 0.9726 - val_recall_6: 0.9380\n",
            "Epoch 6/25\n",
            "1012/1012 [==============================] - 4s 4ms/step - loss: 0.0832 - accuracy: 0.9687 - precision_6: 0.9670 - recall_6: 0.9689 - val_loss: 0.0968 - val_accuracy: 0.9625 - val_precision_6: 0.9572 - val_recall_6: 0.9664\n",
            "Epoch 7/25\n",
            "1012/1012 [==============================] - 4s 4ms/step - loss: 0.0722 - accuracy: 0.9725 - precision_6: 0.9701 - recall_6: 0.9738 - val_loss: 0.0962 - val_accuracy: 0.9647 - val_precision_6: 0.9541 - val_recall_6: 0.9747\n",
            "Epoch 8/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0668 - accuracy: 0.9747 - precision_6: 0.9726 - recall_6: 0.9756 - val_loss: 0.1049 - val_accuracy: 0.9595 - val_precision_6: 0.9503 - val_recall_6: 0.9679\n",
            "Epoch 9/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0622 - accuracy: 0.9764 - precision_6: 0.9744 - recall_6: 0.9774 - val_loss: 0.1322 - val_accuracy: 0.9552 - val_precision_6: 0.9412 - val_recall_6: 0.9690\n",
            "Epoch 10/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0575 - accuracy: 0.9786 - precision_6: 0.9773 - recall_6: 0.9788 - val_loss: 0.0953 - val_accuracy: 0.9636 - val_precision_6: 0.9683 - val_recall_6: 0.9568\n",
            "Epoch 11/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0538 - accuracy: 0.9801 - precision_6: 0.9790 - recall_6: 0.9802 - val_loss: 0.1026 - val_accuracy: 0.9620 - val_precision_6: 0.9674 - val_recall_6: 0.9545\n",
            "Epoch 12/25\n",
            "1012/1012 [==============================] - 4s 4ms/step - loss: 0.0505 - accuracy: 0.9810 - precision_6: 0.9797 - recall_6: 0.9815 - val_loss: 0.1016 - val_accuracy: 0.9612 - val_precision_6: 0.9636 - val_recall_6: 0.9568\n",
            "Epoch 13/25\n",
            "1012/1012 [==============================] - 5s 4ms/step - loss: 0.0493 - accuracy: 0.9818 - precision_6: 0.9806 - recall_6: 0.9821 - val_loss: 0.1079 - val_accuracy: 0.9620 - val_precision_6: 0.9779 - val_recall_6: 0.9437\n",
            "Epoch 14/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0453 - accuracy: 0.9839 - precision_6: 0.9833 - recall_6: 0.9837 - val_loss: 0.1132 - val_accuracy: 0.9595 - val_precision_6: 0.9598 - val_recall_6: 0.9574\n",
            "Epoch 15/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0406 - accuracy: 0.9843 - precision_6: 0.9833 - recall_6: 0.9844 - val_loss: 0.1040 - val_accuracy: 0.9640 - val_precision_6: 0.9630 - val_recall_6: 0.9633\n",
            "Epoch 16/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0390 - accuracy: 0.9849 - precision_6: 0.9843 - recall_6: 0.9847 - val_loss: 0.0952 - val_accuracy: 0.9672 - val_precision_6: 0.9635 - val_recall_6: 0.9696\n",
            "Epoch 17/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0392 - accuracy: 0.9856 - precision_6: 0.9846 - recall_6: 0.9860 - val_loss: 0.0930 - val_accuracy: 0.9691 - val_precision_6: 0.9632 - val_recall_6: 0.9741\n",
            "Epoch 18/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0340 - accuracy: 0.9875 - precision_6: 0.9872 - recall_6: 0.9872 - val_loss: 0.1036 - val_accuracy: 0.9648 - val_precision_6: 0.9722 - val_recall_6: 0.9554\n",
            "Epoch 19/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0339 - accuracy: 0.9869 - precision_6: 0.9862 - recall_6: 0.9871 - val_loss: 0.1157 - val_accuracy: 0.9638 - val_precision_6: 0.9636 - val_recall_6: 0.9625\n",
            "Epoch 20/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0304 - accuracy: 0.9886 - precision_6: 0.9883 - recall_6: 0.9883 - val_loss: 0.1179 - val_accuracy: 0.9665 - val_precision_6: 0.9675 - val_recall_6: 0.9639\n",
            "Epoch 21/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0303 - accuracy: 0.9889 - precision_6: 0.9879 - recall_6: 0.9893 - val_loss: 0.1215 - val_accuracy: 0.9632 - val_precision_6: 0.9585 - val_recall_6: 0.9664\n",
            "Epoch 22/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0272 - accuracy: 0.9902 - precision_6: 0.9900 - recall_6: 0.9900 - val_loss: 0.1188 - val_accuracy: 0.9659 - val_precision_6: 0.9580 - val_recall_6: 0.9730\n",
            "Epoch 23/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0264 - accuracy: 0.9903 - precision_6: 0.9897 - recall_6: 0.9904 - val_loss: 0.1141 - val_accuracy: 0.9661 - val_precision_6: 0.9537 - val_recall_6: 0.9781\n",
            "Epoch 24/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0265 - accuracy: 0.9902 - precision_6: 0.9897 - recall_6: 0.9902 - val_loss: 0.1259 - val_accuracy: 0.9673 - val_precision_6: 0.9533 - val_recall_6: 0.9812\n",
            "Epoch 25/25\n",
            "1012/1012 [==============================] - 5s 5ms/step - loss: 0.0245 - accuracy: 0.9913 - precision_6: 0.9906 - recall_6: 0.9915 - val_loss: 0.1056 - val_accuracy: 0.9690 - val_precision_6: 0.9588 - val_recall_6: 0.9787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f65560d3e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysagveaBRI3i",
        "outputId": "899766b7-ae19-447e-c6a9-f66d3b7a9a78"
      },
      "source": [
        "loss, accuracy, precision, recall = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "print(\"Training Precision:  {:.4f}\".format(precision))\n",
        "print(\"Training Recall:  {:.4f}\".format(recall))\n",
        "\n",
        "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "print(\"Testing Precision:  {:.4f}\".format(precision))\n",
        "print(\"Testing Recall:  {:.4f}\".format(recall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9931\n",
            "Training Precision:  0.9890\n",
            "Training Recall:  0.9969\n",
            "Testing Accuracy:  0.9711\n",
            "Testing Precision:  0.9624\n",
            "Testing Recall:  0.9792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW2JdbDvlZuQ"
      },
      "source": [
        "model.save(\"NN_model_embeddings_final.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtddY0QMRI3k"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "F1_Score_Test = 2 *( ( precision * recall) / (precision + recall))\n",
        "\n",
        "metrics_array = np.array([[accuracy, precision], [recall, F1_Score_Test]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Db5pxBjORI3m",
        "outputId": "7b5705c8-46c4-46e4-dbda-f09cd7c44515"
      },
      "source": [
        "group_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "group_counts = [\"{0:0.3f}\".format(value) for value in metrics_array.flatten()]\n",
        "\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names, group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "sns.heatmap(metrics_array, annot=labels, fmt='', cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6554b2d2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMwQlBttcfIo",
        "outputId": "925e2ae7-01d1-48a1-da8a-70dcd635e842"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzlilRJhbbA4",
        "outputId": "7e3ee701-0ea3-468f-bc3b-b3c287081b19"
      },
      "source": [
        "# Save model locally\n",
        "#model_save_name = 'NN_model_Keras_Embedding_layer'\n",
        "#path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "#model.save(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO : Assets written to: /content/gdrive/My Drive/NN_model_Keras_Embedding_layer/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoqExzQzRI3p"
      },
      "source": [
        "## New model but with no embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDDeSs69eHLF",
        "outputId": "c4d9cd40-2428-4097-c6d3-808edf50fdd9"
      },
      "source": [
        "import pandas as pd\n",
        "# Pulling data from Hussein's github repo\n",
        "!wget \"https://raw.githubusercontent.com/HAadams/fake_news_dataset/master/combined_data.zip\" -O combined_data.zip\n",
        "!unzip combined_data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-20 22:49:11--  https://raw.githubusercontent.com/HAadams/fake_news_dataset/master/combined_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 71497977 (68M) [application/zip]\n",
            "Saving to: ‘combined_data.zip’\n",
            "\n",
            "combined_data.zip   100%[===================>]  68.19M   114MB/s    in 0.6s    \n",
            "\n",
            "2020-11-20 22:49:12 (114 MB/s) - ‘combined_data.zip’ saved [71497977/71497977]\n",
            "\n",
            "Archive:  combined_data.zip\n",
            "  inflating: combined_data.csv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2hxKaGlYa6m"
      },
      "source": [
        "df = pd.read_csv('combined_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyIwhN-JRI3p"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# list of text documents\n",
        "text = df['Text']\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLN3FkWLRI3r"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "text = df['Text']\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_vector = vectorizer.fit_transform(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK6rdDWERI3t"
      },
      "source": [
        "text = df['Text'].tolist()\n",
        "y = df['Target'].tolist()\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUBiiwXjRI3w"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tf_vector, y, test_size = 0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A39_pM4zYhvT",
        "outputId": "9cb82a3e-e798-4a29-f2fa-ad08d2adc758"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17980, 221412)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id2SgKhYY1QZ",
        "outputId": "b8c70393-3cf5-4eb7-f6ff-4b9c93a52d8d"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(71918, 221412)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9XWuMn0sOJ6",
        "outputId": "6a57fb7b-d1b0-4ec9-c1a2-62066900ae49"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "max_features = 230000 \n",
        "\n",
        "model.add(layers.Dense(2, activation='relu', input_shape=(X_train.shape[1],) ) )\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(.4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(.4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(.4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(.4))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(.4))\n",
        "model.add(Dense(64, activation='relu')) # 4 dense layers\n",
        "model.add(layers.Dropout(.4))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss =\"binary_crossentropy\", optimizer='adam', metrics = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 2)                 442826    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                192       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 463,883\n",
            "Trainable params: 463,883\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3idKuqRZsOeL"
      },
      "source": [
        "import numpy as np\n",
        "# Due to large feature space, each document must be individually trained for model\n",
        "for item in range(X_train.shape[0]):\n",
        "  model.fit(X_train[item].toarray(), np.array([y_train[item]]), epochs = 20, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuRqimXAtHfN"
      },
      "source": [
        "# Predict function gives continuous numerical output\n",
        "y_pred = []\n",
        "for item in range(X_test.shape[0]):\n",
        "    y_pred.append(model.predict(X_test[item].toarray(), verbose=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKkvrFs4tHqC"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "# Map continuous output to binary values\n",
        "newList = [item.tolist()[0][0] for item in np.array(y_pred)]\n",
        "y_pred = np.asarray(newList)\n",
        "y_pred[y_pred <= 0.5] = 0\n",
        "y_pred[y_pred > 0.5] = 1\n",
        "\n",
        "wo_embed_nn_acc = accuracy_score(y_test, y_pred)\n",
        "wo_embed_nn_prec = precision_score(y_test, y_pred, average='macro')\n",
        "wo_embed_nn_recall = recall_score(y_test, y_pred, average='macro')\n",
        "print('Accuracy:\\t', wo_embed_nn_acc)\n",
        "print('Precision:\\t', wo_embed_nn_prec)\n",
        "print('Recall:\\t\\t', wo_embed_nn_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhWElNb3DZUs"
      },
      "source": [
        "model.save(\"NN_model_no_embeddings_final.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}